{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "# A little bit of something to process Korean\n",
    "import unicodedata\n",
    "import string\n",
    "import hanja\n",
    "from hanja import hangul\n",
    "\n",
    "def fix_unicode(s):\n",
    "    return ''.join(x for x in unicodedata.normalize('NFC', s))\n",
    "\n",
    "stopwords = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', \n",
    "             'ㄽ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', \n",
    "             'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅓ', 'ㅔ', 'ㅕ', \n",
    "             'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅠ', 'ㅡ', \n",
    "             'ㅢ', 'ㅣ', 'ㅻ', 'ㆍ', '\\t', ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data2Dataset(Dataset):\n",
    "    def __init__(self, data, label, max_len):\n",
    "        self.X = data\n",
    "        self.y = label\n",
    "        self.max_len = max_len\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.X[idx]\n",
    "        batch_y = self.y[idx]        \n",
    "        if len(batch_x) > self.max_len:\n",
    "            batch_x = batch_x[:self.max_len]\n",
    "            batch_x = [char2idx.get(x, char2idx['UNK']) for x in batch_x]\n",
    "            position = torch.arange(0,self.max_len)\n",
    "        else:\n",
    "            batch_x = [d for d in batch_x]\n",
    "            len_x = len(batch_x)\n",
    "            batch_x = batch_x + ['PAD'] * (self.max_len-len_x)\n",
    "            batch_x = [char2idx.get(x, char2idx['UNK']) for x in batch_x]\n",
    "            position = F.pad(torch.arange(0, len_x), (0, self.max_len-len_x), 'constant', 0)\n",
    "        batch_y = label2idx[batch_y]\n",
    "        \n",
    "        return torch.tensor(batch_x).long(), torch.tensor(batch_y).long(), position.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=100):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, embedding_dim).float()\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, embedding_dim, 2).float()*\n",
    "                    -(math.log(10000.0) / embedding_dim)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.pos_embed = nn.Embedding.from_pretrained(pe, freeze=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pos_embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position = PositionalEmbedding(embedding_dim=self.token.embedding_dim, max_len=max_len)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x, p):\n",
    "        p = self.position(p)\n",
    "        x = self.token(x)\n",
    "        x = x + p\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rumble(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding = TokenEmbedding(vocab_size=self.vocab_size, \n",
    "                                        embedding_dim=self.embedding_dim, \n",
    "                                        max_len=self.max_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, \n",
    "                                                   nhead=num_heads, \n",
    "                                                   dim_feedforward=1024, \n",
    "                                                   dropout=0.1, \n",
    "                                                   activation='gelu')\n",
    "        layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, \n",
    "                                             num_layers=num_layers, \n",
    "                                             norm=layer_norm)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, p):\n",
    "        x = self.embedding(x, p)\n",
    "        x = self.encoder(x)\n",
    "        x = self.linear(x)               \n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Soundwave(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "        self.linear = nn.Linear(embedding_dim*max_len, label_size)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.embedding_dim * self.max_len)\n",
    "        x = self.linear(self.dropout(x))             \n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_pair.pickle', 'rb') as pp:\n",
    "    data = pickle.load(pp)\n",
    "\n",
    "texts = [''.join(fix_unicode(x).split()) for y, x in data]\n",
    "text_filtered = []\n",
    "for text in texts:\n",
    "    text = [x for x in text if (x in string.printable) or (hangul.is_hangul(x))]\n",
    "    text = [x for x in text if x not in stopwords]\n",
    "    text_filtered.append(text)\n",
    "\n",
    "labels = [fix_unicode(y) for y, x in data]\n",
    "label2idx = {x: c for c, x in enumerate(sorted(labels))}\n",
    "idx2label = {c: x for c, x in enumerate(sorted(labels))}\n",
    "\n",
    "with open('char2idx_wMASK.pickle', 'rb') as pp:\n",
    "    char2idx = pickle.load(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char2idx)\n",
    "label_size = len(idx2label)\n",
    "embedding_dim = 256\n",
    "max_len = 40\n",
    "num_layers = 8\n",
    "num_heads = 8\n",
    "batch_size = 32\n",
    "lr = 2e-5\n",
    "\n",
    "train_dataset = data2Dataset(text_filtered, labels, max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          num_workers=12, shuffle=True, pin_memory=True)\n",
    "\n",
    "rumble = Rumble(vocab_size=vocab_size, embedding_dim=embedding_dim, max_len=max_len, \n",
    "                     num_layers=num_layers, num_heads=num_heads).to(device)\n",
    "rumble_path = '/path/to/rumble/model/file.pt'\n",
    "rumble.load_state_dict(torch.load(rumble_path))\n",
    "\n",
    "soundwave = Soundwave(vocab_size=vocab_size, embedding_dim=embedding_dim, \n",
    "                      max_len=max_len).to(device)\n",
    "optimizer = optim.Adam(soundwave.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "loss_func = nn.NLLLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 2\n",
    "plot_train = []\n",
    "beg = time.time()\n",
    "tot_length = len(train_loader)\n",
    "soundwave.train()\n",
    "for epoch in range(num_epoch):    \n",
    "    train_loss = 0\n",
    "    for e, d in enumerate(train_loader):\n",
    "        X, y, p = d\n",
    "        X, y, p = X.to(device), y.to(device), p.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        feature = rumble.embedding(X,p)\n",
    "        feature = rumble.encoder(feature)\n",
    "        \n",
    "        y_hat = soundwave(feature)\n",
    "        loss = loss_func(y_hat, y)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter_num = e + epoch*tot_length\n",
    "        \n",
    "        if iter_num%250 == 1:\n",
    "            print('loss at: {}, {:.3f}, time so far: {}'.format(\n",
    "                iter_num, train_loss/e, time.time()-beg))\n",
    "            torch.save(soundwave.state_dict(), \n",
    "                       'Soundwave_{iter}_iter.pt'.format(iter=iter_num))\n",
    "            plot_train.append(train_loss/e)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
