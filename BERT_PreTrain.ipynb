{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "# A little bit of something to process Korean\n",
    "import unicodedata\n",
    "import string\n",
    "import hanja\n",
    "from hanja import hangul\n",
    "\n",
    "def fix_unicode(s):\n",
    "    return ''.join(x for x in unicodedata.normalize('NFC', s))\n",
    "\n",
    "stopwords = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', \n",
    "             'ㄽ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', \n",
    "             'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅓ', 'ㅔ', 'ㅕ', \n",
    "             'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅠ', 'ㅡ', \n",
    "             'ㅢ', 'ㅣ', 'ㅻ', 'ㆍ', '\\t', ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        replaced, label = self.pick_random(sample)\n",
    "        if len(replaced) < max_len: # When shorter than max_len\n",
    "            position = np.pad(np.arange(0, len(replaced)), \n",
    "                              (0, max_len-len(replaced)), \n",
    "                              mode='constant')\n",
    "            replaced = replaced + [char2idx['PAD']] * (max_len - len(replaced))\n",
    "            label = label + [char2idx['PAD']] * (max_len - len(label))        \n",
    "        elif len(replaced) >= max_len:\n",
    "            replaced = replaced[:max_len]\n",
    "            label = label[:max_len]\n",
    "            position = np.arange(0, max_len)\n",
    "        \n",
    "        return torch.tensor(replaced), position, torch.tensor(label)\n",
    "        \n",
    "        \n",
    "    def pick_random(self, sample):\n",
    "        label = []\n",
    "        replaced = []\n",
    "        for e, char in enumerate(sample):\n",
    "            q_pick = random.random()\n",
    "            if q_pick <= 0.15:\n",
    "                toss = random.random()\n",
    "                \n",
    "                if toss <= 0.80:\n",
    "                    replaced.append(char2idx['MASK'])\n",
    "                    \n",
    "                elif toss <= 0.90:\n",
    "                    replaced.append(char2idx[list(char2idx.keys())[random.randint(0, len(char2idx)-1)]])\n",
    "                    \n",
    "                else:\n",
    "                    replaced.append(char2idx.get(sample[e], char2idx['UNK']))\n",
    "            \n",
    "                label.append(char2idx.get(char, char2idx['UNK']))\n",
    "            \n",
    "            else:\n",
    "                replaced.append(char2idx.get(sample[e], char2idx['UNK']))\n",
    "                label.append(0)\n",
    "            \n",
    "        return replaced, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, embedding_dim).float()\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, embedding_dim, 2).float()*\n",
    "                    -(math.log(10000.0)/embedding_dim)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.pos_embed = nn.Embedding.from_pretrained(pe, freeze=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pos_embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.position = PositionalEmbedding(embedding_dim=self.token.embedding_dim, \n",
    "                                            max_len=max_len)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x, p):\n",
    "        p = self.position(p)\n",
    "        x = self.token(x)\n",
    "        x = x + p\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rumble(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding = TokenEmbedding(vocab_size=self.vocab_size, \n",
    "                                        embedding_dim=self.embedding_dim, \n",
    "                                        max_len=self.max_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, \n",
    "                                                   nhead=num_heads, \n",
    "                                                   dim_feedforward=1024, \n",
    "                                                   dropout=0.1, \n",
    "                                                   activation='gelu')\n",
    "        layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, \n",
    "                                             num_layers=num_layers, \n",
    "                                             norm=layer_norm)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, p):\n",
    "        x = self.embedding(x, p)\n",
    "        x = self.encoder(x)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data is a list of names\n",
    "with open('data.pickle', 'rb') as pp:\n",
    "    data = pickle.load(pp)\n",
    "\n",
    "data = [''.join(fix_unicode(x).split()) for x in data]\n",
    "data_processed = []\n",
    "for d in data:\n",
    "    d = [x for x in d if (x in string.printable) or (hangul.is_hangul(x))]\n",
    "    d = [x for x in d if x not in stopwords]\n",
    "    data_processed.append(d)\n",
    "\n",
    "vocab = ''.join(stores_filtered)\n",
    "vocab = set(vocab)\n",
    "    \n",
    "char2idx = {}\n",
    "char2idx['PAD'] = 0\n",
    "char2idx['UNK'] = 1\n",
    "char2idx['MASK'] = 2\n",
    "char2idx.update({x: c+len(char2idx) for c, x in enumerate(sorted(vocab))})\n",
    "\n",
    "with open('char2idx.pickle', 'wb') as pp:\n",
    "    pickle.dump(char2idx, pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char2idx)\n",
    "embedding_dim = 256\n",
    "max_len = 40\n",
    "num_layers = 8\n",
    "num_heads = 8\n",
    "batch_size = 500\n",
    "lr = 1e-4\n",
    "\n",
    "train_dataset = MLMData(data_processed)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          num_workers=12, shuffle=True, pin_memory=True)\n",
    "rumble = Rumble(vocab_size=vocab_size, embedding_dim=embedding_dim, max_len=max_len, \n",
    "                      num_layers=num_layers, num_heads=num_heads).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(rumble.parameters(), lr=lr)\n",
    "loss_func = nn.NLLLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 50\n",
    "plot_train = []\n",
    "beg = time.time()\n",
    "tot_length = len(train_loader)\n",
    "for epoch in range(num_epoch):\n",
    "    rumble.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    for e, d in enumerate(train_loader):\n",
    "        X, p, y = d\n",
    "        X, p, y = X.to(device), p.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = rumble(X, p)\n",
    "        loss = loss_func(y_hat.transpose(1,2), y)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter_num = e + epoch*tot_length\n",
    "        \n",
    "        if iter_num%250 == 1:\n",
    "            print('loss at: {}, {:.3f}, time so far: {}'.format(\n",
    "                iter_num, train_loss/e, time.time()-beg))\n",
    "            plot_train.append(train_loss/e)\n",
    "            \n",
    "        if iter_num%3000 == 2999:\n",
    "            torch.save(rumble.state_dict(), \n",
    "                       'rumble_{e}_iter.pt'.format(e=iter_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
